[2013-01-18 18:31:01,098][INFO ][node                     ] [Assassin] {0.20.2}[4884]: initializing ...
[2013-01-18 18:31:01,098][DEBUG][node                     ] [Assassin] using home [C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2], config [C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2\config], data [[C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2\data]], logs [C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2\logs], work [C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2\work], plugins [C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2\plugins]
[2013-01-18 18:31:01,098][INFO ][plugins                  ] [Assassin] loaded [], sites []
[2013-01-18 18:31:01,114][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder
[2013-01-18 18:31:01,285][TRACE][env                      ] [Assassin] obtaining node lock on C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2\data\whalewired_cluster\nodes\0 ...
[2013-01-18 18:31:01,285][DEBUG][env                      ] [Assassin] using node location [[C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2\data\whalewired_cluster\nodes\0]], local_node_id [0]
[2013-01-18 18:31:01,285][TRACE][env                      ] [Assassin] node data locations details:
 -> C:\dev\git\whalewired\whalewired.tools\elasticsearch-0.20.2\data\whalewired_cluster\nodes\0, free_space [26.7gb], usable_space [26.7gb]

[2013-01-18 18:31:01,800][TRACE][monitor.sigar            ] [Assassin] sigar loaded successfully
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [generic], type [cached], keep_alive [30s]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [index], type [cached], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [bulk], type [cached], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [get], type [cached], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [search], type [cached], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [percolate], type [cached], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [flush], type [scaling], min [1], size [10], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [merge], type [scaling], min [1], size [20], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [refresh], type [scaling], min [1], size [10], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [cache], type [scaling], min [1], size [4], keep_alive [5m]
[2013-01-18 18:31:02,237][DEBUG][threadpool               ] [Assassin] creating thread_pool [snapshot], type [scaling], min [1], size [5], keep_alive [5m]
[2013-01-18 18:31:02,253][DEBUG][transport.netty          ] [Assassin] using worker_count[16], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/6/1], receive_predictor[512kb->512kb]
[2013-01-18 18:31:02,268][DEBUG][discovery.zen.ping.multicast] [Assassin] using group [224.2.2.4], with port [54328], ttl [3], and address [null]
[2013-01-18 18:31:02,268][DEBUG][discovery.zen.ping.unicast] [Assassin] using initial hosts [], with concurrent_connects [10]
[2013-01-18 18:31:02,268][DEBUG][discovery.zen            ] [Assassin] using ping.timeout [3s], master_election.filter_client [true], master_election.filter_data [false]
[2013-01-18 18:31:02,268][DEBUG][discovery.zen.elect      ] [Assassin] using minimum_master_nodes [-1]
[2013-01-18 18:31:02,268][DEBUG][discovery.zen.fd         ] [Assassin] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2013-01-18 18:31:02,284][DEBUG][discovery.zen.fd         ] [Assassin] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2013-01-18 18:31:02,299][DEBUG][monitor.jvm              ] [Assassin] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, ParNew=GcThreshold{name='ParNew', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, ConcurrentMarkSweep=GcThreshold{name='ConcurrentMarkSweep', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}]
[2013-01-18 18:31:02,814][DEBUG][monitor.os               ] [Assassin] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@5fe2cae3] with refresh_interval [1s]
[2013-01-18 18:31:02,830][DEBUG][monitor.process          ] [Assassin] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@6040b6ef] with refresh_interval [1s]
[2013-01-18 18:31:02,845][DEBUG][monitor.jvm              ] [Assassin] Using refresh_interval [1s]
[2013-01-18 18:31:02,845][DEBUG][monitor.network          ] [Assassin] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@2aed913b] with refresh_interval [5s]
[2013-01-18 18:31:03,470][DEBUG][monitor.network          ] [Assassin] net_info
host [9C-CLA]
lo	display_name [Software Loopback Interface 1]
		address [/127.0.0.1] [/0:0:0:0:0:0:0:1] 
		mtu [-1] multicast [true] ptp [false] loopback [true] up [true] virtual [false]
net0	display_name [WAN Miniport (SSTP)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net1	display_name [WAN Miniport (L2TP)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net2	display_name [WAN Miniport (PPTP)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
ppp0	display_name [WAN Miniport (PPPOE)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth0	display_name [WAN Miniport (IPv6)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth1	display_name [WAN Miniport (Network Monitor)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth2	display_name [WAN Miniport (IP)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
ppp1	display_name [RAS Async Adapter]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net3	display_name [WAN Miniport (IKEv2)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth3	display_name [Bluetooth Device (Personal Area Network)]
		address [/fe80:0:0:0:d1c9:2348:1817:68e2%11] 
		mtu [1500] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net4	display_name [Bluetooth Device (RFCOMM Protocol TDI)]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth4	display_name [Realtek PCIe GBE Family Controller]
		address [/192.168.1.34] [/fe80:0:0:0:2454:76cb:6d28:c13e%13] 
		mtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]
net5	display_name [Teredo Tunneling Pseudo-Interface]
		address [/fe80:0:0:0:e0:0:0:0%14] 
		mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]
net6	display_name [Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC]
		address [/fe80:0:0:0:7864:72fa:c325:2c08%15] 
		mtu [1500] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth5	display_name [WAN Miniport (Network Monitor) - Deterministic Network Enhancer Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth6	display_name [WAN Miniport (IP) - Deterministic Network Enhancer Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth7	display_name [Realtek PCIe GBE Family Controller - Deterministic Network Enhancer Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth8	display_name [Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC - Deterministic Network Enhancer Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth9	display_name [Cisco Systems VPN Adapter for 64-bit Windows]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth10	display_name [Cisco Systems VPN Adapter for 64-bit Windows - Deterministic Network Enhancer Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net7	display_name [Microsoft Virtual WiFi Miniport Adapter]
		address [/fe80:0:0:0:9d9d:6757:30db:b9ab%22] 
		mtu [1500] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth11	display_name [Microsoft Virtual WiFi Miniport Adapter - Deterministic Network Enhancer Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth12	display_name [Microsoft Virtual WiFi Miniport Adapter - VirtualBox Bridged Networking Driver Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth13	display_name [Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC - VirtualBox Bridged Networking Driver Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth14	display_name [Realtek PCIe GBE Family Controller - VirtualBox Bridged Networking Driver Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth15	display_name [VirtualBox Host-Only Ethernet Adapter]
		address [/192.168.56.1] [/fe80:0:0:0:102e:76b:a835:3494%27] 
		mtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]
eth16	display_name [VirtualBox Host-Only Ethernet Adapter - Deterministic Network Enhancer Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth17	display_name [Cisco Systems VPN Adapter for 64-bit Windows - VirtualBox Bridged Networking Driver Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth18	display_name [Juniper Network Connect Virtual Adapter]
		address [/fe80:0:0:0:c557:175a:c38b:ef98%30] 
		mtu [1400] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth19	display_name [Juniper Network Connect Virtual Adapter - Deterministic Network Enhancer Miniport]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth20	display_name [WAN Miniport (Network Monitor) - Deterministic Network Enhancer Miniport-QoS Packet Scheduler-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net8	display_name [Microsoft ISATAP Adapter #2]
		address 
		mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]
net9	display_name [Microsoft ISATAP Adapter]
		address 
		mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]
net10	display_name [Microsoft ISATAP Adapter #6]
		address [/fe80:0:0:0:0:5efe:c0a8:122%35] 
		mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]
net11	display_name [Microsoft ISATAP Adapter #4]
		address 
		mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]
net12	display_name [Microsoft ISATAP Adapter #3]
		address 
		mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]
net13	display_name [Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC-Virtual WiFi Filter Driver-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth21	display_name [VirtualBox Host-Only Ethernet Adapter - Deterministic Network Enhancer Miniport-QoS Packet Scheduler-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth22	display_name [VirtualBox Host-Only Ethernet Adapter - Deterministic Network Enhancer Miniport-WFP LightWeight Filter-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth23	display_name [WAN Miniport (IPv6)-QoS Packet Scheduler-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth24	display_name [WAN Miniport (IP) - Deterministic Network Enhancer Miniport-QoS Packet Scheduler-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth25	display_name [Realtek PCIe GBE Family Controller - VirtualBox Bridged Networking Driver Miniport-QoS Packet Scheduler-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth26	display_name [Realtek PCIe GBE Family Controller - VirtualBox Bridged Networking Driver Miniport-WFP LightWeight Filter-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth27	display_name [Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC - VirtualBox Bridged Networking Driver Miniport-QoS Packet Scheduler-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net14	display_name [Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC-Native WiFi Filter Driver-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth28	display_name [Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC - VirtualBox Bridged Networking Driver Miniport-WFP LightWeight Filter-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth29	display_name [Microsoft Virtual WiFi Miniport Adapter - VirtualBox Bridged Networking Driver Miniport-QoS Packet Scheduler-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net15	display_name [Microsoft Virtual WiFi Miniport Adapter-Native WiFi Filter Driver-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth30	display_name [Microsoft Virtual WiFi Miniport Adapter - VirtualBox Bridged Networking Driver Miniport-WFP LightWeight Filter-0000]
		address 
		mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net16	display_name [Microsoft ISATAP Adapter #5]
		address 
		mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]

[2013-01-18 18:31:03,579][TRACE][monitor.network          ] [Assassin] ifconfig

WAN Miniport (IPv6)
eth0	Link encap:Ethernet HWaddr 14:0C:20:52:41:53
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
WAN Miniport (Network Monitor)
eth1	Link encap:Ethernet HWaddr 14:0C:20:52:41:53
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
WAN Miniport (Network Monitor) - Deterministic Network Enhancer Miniport
eth2	Link encap:Ethernet HWaddr 14:0C:20:52:41:53
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
WAN Miniport (Network Monitor) - Deterministic Network Enhancer Miniport-QoS Packet Scheduler-0000
eth3	Link encap:Ethernet HWaddr 14:0C:20:52:41:53
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
WAN Miniport (IP)
eth4	Link encap:Ethernet HWaddr 14:0C:20:52:41:53
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
VirtualBox Host-Only Ethernet Adapter - Deterministic Network Enhancer Miniport-QoS Packet Scheduler-0000
eth5	Link encap:Ethernet HWaddr 08:00:27:00:44:75
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:2050 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:426901 (417K)
Bluetooth Device (Personal Area Network)
eth6	Link encap:Ethernet HWaddr 44:6D:57:76:37:EE
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek PCIe GBE Family Controller
eth7	Link encap:Ethernet HWaddr 00:90:F5:CF:D8:5E
	inet addr:192.168.1.34  Bcast:192.168.1.255  Mask:255.255.255.0
	UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:0
	RX packets:19194 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:10981 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:22719538 ( 22M)  TX bytes:1140146 (1.1M)
WAN Miniport (IP) - Deterministic Network Enhancer Miniport
eth8	Link encap:Ethernet HWaddr 14:0C:20:52:41:53
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
WAN Miniport (IPv6)-QoS Packet Scheduler-0000
eth9	Link encap:Ethernet HWaddr 14:0C:20:52:41:53
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek PCIe GBE Family Controller - Deterministic Network Enhancer Miniport
eth10	Link encap:Ethernet HWaddr 00:90:F5:CF:D8:5E
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:19194 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:10981 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:22719538 ( 22M)  TX bytes:1140146 (1.1M)
VirtualBox Host-Only Ethernet Adapter - Deterministic Network Enhancer Miniport-WFP LightWeight Filter-0000
eth11	Link encap:Ethernet HWaddr 08:00:27:00:44:75
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:2050 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:426901 (417K)
WAN Miniport (IP) - Deterministic Network Enhancer Miniport-QoS Packet Scheduler-0000
eth12	Link encap:Ethernet HWaddr 14:0C:20:52:41:53
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC - Deterministic Network Enhancer Miniport
eth13	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Microsoft Virtual WiFi Miniport Adapter - VirtualBox Bridged Networking Driver Miniport
eth14	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek PCIe GBE Family Controller - VirtualBox Bridged Networking Driver Miniport-QoS Packet Scheduler-0000
eth15	Link encap:Ethernet HWaddr 00:90:F5:CF:D8:5E
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:19197 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:10983 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:22719598 ( 22M)  TX bytes:1140146 (1.1M)
Cisco Systems VPN Adapter for 64-bit Windows
eth16	Link encap:Ethernet HWaddr 00:05:9A:3C:78:00
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:0  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Cisco Systems VPN Adapter for 64-bit Windows - Deterministic Network Enhancer Miniport
eth17	Link encap:Ethernet HWaddr 00:05:9A:3C:78:00
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:0  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Microsoft Virtual WiFi Miniport Adapter - Deterministic Network Enhancer Miniport
eth18	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek PCIe GBE Family Controller - VirtualBox Bridged Networking Driver Miniport-WFP LightWeight Filter-0000
eth19	Link encap:Ethernet HWaddr 00:90:F5:CF:D8:5E
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:19197 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:10985 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:22719828 ( 22M)  TX bytes:1141941 (1.1M)
Juniper Network Connect Virtual Adapter
eth20	Link encap:Ethernet HWaddr 00:FF:20:1D:D9:08
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1400  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC - VirtualBox Bridged Networking Driver Miniport-QoS Packet Scheduler-0000
eth21	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC - VirtualBox Bridged Networking Driver Miniport-WFP LightWeight Filter-0000
eth22	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Juniper Network Connect Virtual Adapter - Deterministic Network Enhancer Miniport
eth23	Link encap:Ethernet HWaddr 00:FF:20:1D:D9:08
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1400  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC - VirtualBox Bridged Networking Driver Miniport
eth24	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Microsoft Virtual WiFi Miniport Adapter - VirtualBox Bridged Networking Driver Miniport-QoS Packet Scheduler-0000
eth25	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Microsoft Virtual WiFi Miniport Adapter - VirtualBox Bridged Networking Driver Miniport-WFP LightWeight Filter-0000
eth26	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Cisco Systems VPN Adapter for 64-bit Windows - VirtualBox Bridged Networking Driver Miniport
eth27	Link encap:Ethernet HWaddr 00:05:9A:3C:78:00
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:0  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek PCIe GBE Family Controller - VirtualBox Bridged Networking Driver Miniport
eth28	Link encap:Ethernet HWaddr 00:90:F5:CF:D8:5E
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:19197 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:10985 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:22719828 ( 22M)  TX bytes:1141941 (1.1M)
VirtualBox Host-Only Ethernet Adapter
eth29	Link encap:Ethernet HWaddr 08:00:27:00:44:75
	inet addr:192.168.56.1  Bcast:192.168.56.255  Mask:255.255.255.0
	UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:2050 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:426901 (417K)
VirtualBox Host-Only Ethernet Adapter - Deterministic Network Enhancer Miniport
eth30	Link encap:Ethernet HWaddr 08:00:27:00:44:75
	inet addr:0.0.0.0  Mask:0.0.0.0
	UP RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:2050 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:426901 (417K)
Software Loopback Interface 1
lo0	Link encap:Local Loopback HWaddr 6F:00:73:00:74:00
	inet addr:127.0.0.1  Mask:255.0.0.0
	UP LOOPBACK RUNNING  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC
eth31	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC-Virtual WiFi Filter Driver-0000
eth32	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Realtek RTL8723AE Wireless LAN 802.11n PCI-E NIC-Native WiFi Filter Driver-0000
eth33	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Microsoft Virtual WiFi Miniport Adapter
eth34	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )
Microsoft Virtual WiFi Miniport Adapter-Native WiFi Filter Driver-0000
eth35	Link encap:Ethernet HWaddr 44:6D:57:76:17:64
	inet addr:0.0.0.0  Mask:0.0.0.0
	[NO FLAGS]  MTU:1500  Metric:0
	RX packets:0 errors:0 dropped:0 overruns:-1 frame:-1
	TX packets:0 errors:0 dropped:0 overruns:-1 carrier:-1
	collisions:-1
	RX bytes:0 (  0 )  TX bytes:0 (  0 )

[2013-01-18 18:31:04,640][DEBUG][monitor.fs               ] [Assassin] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@7e6c57f4] with refresh_interval [1s]
[2013-01-18 18:31:04,827][DEBUG][indices.store            ] [Assassin] using indices.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]
[2013-01-18 18:31:04,827][DEBUG][cache.memory             ] [Assassin] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]
[2013-01-18 18:31:04,858][DEBUG][cluster.routing.allocation.decider] [Assassin] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]
[2013-01-18 18:31:04,858][DEBUG][cluster.routing.allocation.decider] [Assassin] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2013-01-18 18:31:04,858][DEBUG][cluster.routing.allocation.decider] [Assassin] using [cluster_concurrent_rebalance] with [2]
[2013-01-18 18:31:04,874][DEBUG][gateway.local            ] [Assassin] using initial_shards [quorum], list_timeout [30s]
[2013-01-18 18:31:04,920][DEBUG][indices.recovery         ] [Assassin] using max_size_per_sec[0b], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2013-01-18 18:31:04,952][DEBUG][http.netty               ] [Assassin] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb->512kb]
[2013-01-18 18:31:04,952][DEBUG][indices.memory           ] [Assassin] using index_buffer_size [98.9mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]
[2013-01-18 18:31:04,967][DEBUG][indices.cache.filter     ] [Assassin] using [node] weighted filter cache with size [20%], actual_size [197.9mb], expire [null], clean_interval [1m]
[2013-01-18 18:31:04,967][DEBUG][gateway.local.state.meta ] [Assassin] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]
[2013-01-18 18:31:04,967][TRACE][gateway.local.state.meta ] [Assassin] [upgrade]: processing [global-9]
[2013-01-18 18:31:05,045][DEBUG][gateway.local.state.meta ] [Assassin] took 78ms to load state
[2013-01-18 18:31:05,045][TRACE][gateway.local.state.shards] [Assassin] [find_latest_state]: processing [global-9]
[2013-01-18 18:31:05,045][DEBUG][gateway.local.state.shards] [Assassin] took 0s to load started shards state
[2013-01-18 18:31:05,045][DEBUG][bulk.udp                 ] [Assassin] using enabled [false], host [null], port [9700-9800], bulk_actions [1000], bulk_size [5mb], flush_interval [5s], concurrent_requests [4]
[2013-01-18 18:31:05,061][INFO ][node                     ] [Assassin] {0.20.2}[4884]: initialized
[2013-01-18 18:31:05,061][INFO ][node                     ] [Assassin] {0.20.2}[4884]: starting ...
[2013-01-18 18:31:05,076][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500
[2013-01-18 18:31:05,076][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false
[2013-01-18 18:31:05,139][DEBUG][transport.netty          ] [Assassin] Bound to address [/0:0:0:0:0:0:0:0:9300]
[2013-01-18 18:31:05,326][INFO ][transport                ] [Assassin] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.34:9300]}
[2013-01-18 18:31:05,716][TRACE][discovery                ] [Assassin] waiting for 30s for the initial state to be set by the discovery
[2013-01-18 18:31:05,716][TRACE][discovery.zen.ping.multicast] [Assassin] [1] sending ping request
[2013-01-18 18:31:07,229][TRACE][discovery.zen.ping.multicast] [Assassin] [1] sending ping request
[2013-01-18 18:31:08,758][TRACE][discovery.zen            ] [Assassin] full ping responses: {none}
[2013-01-18 18:31:08,758][DEBUG][discovery.zen            ] [Assassin] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2013-01-18 18:31:08,758][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-join (elected_as_master)]: execute
[2013-01-18 18:31:08,758][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [1], source [zen-disco-join (elected_as_master)]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
---- unassigned

[2013-01-18 18:31:08,758][INFO ][cluster.service          ] [Assassin] new_master [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], reason: zen-disco-join (elected_as_master)
[2013-01-18 18:31:08,789][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x811a3216, /192.168.1.34:55252 => /192.168.1.34:9300]
[2013-01-18 18:31:08,789][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x3d96c8e5, /192.168.1.34:55253 => /192.168.1.34:9300]
[2013-01-18 18:31:08,789][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x644b2965, /192.168.1.34:55254 => /192.168.1.34:9300]
[2013-01-18 18:31:08,789][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x05a9104d, /192.168.1.34:55255 => /192.168.1.34:9300]
[2013-01-18 18:31:08,789][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x0adb2d76, /192.168.1.34:55256 => /192.168.1.34:9300]
[2013-01-18 18:31:08,805][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x62a33fb8, /192.168.1.34:55257 => /192.168.1.34:9300]
[2013-01-18 18:31:08,805][DEBUG][transport.netty          ] [Assassin] connected to node [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]]
[2013-01-18 18:31:08,805][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x927c4c87, /192.168.1.34:55258 => /192.168.1.34:9300]
[2013-01-18 18:31:08,805][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x61971f4e, /192.168.1.34:55259 => /192.168.1.34:9300]
[2013-01-18 18:31:08,805][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x445cae4f, /192.168.1.34:55260 => /192.168.1.34:9300]
[2013-01-18 18:31:08,805][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 18:31:08,805][TRACE][discovery                ] [Assassin] initial state set from discovery
[2013-01-18 18:31:08,805][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-join (elected_as_master)]: done applying updated cluster_state
[2013-01-18 18:31:08,805][INFO ][discovery                ] [Assassin] whalewired_cluster/tJdQcBaOToW8ejPALNbsGQ
[2013-01-18 18:31:08,805][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 18:31:08,805][TRACE][gateway                  ] [Assassin] performing state recovery...
[2013-01-18 18:31:08,805][TRACE][gateway.local            ] [Assassin] performing state recovery from [tJdQcBaOToW8ejPALNbsGQ]
[2013-01-18 18:31:08,820][TRACE][gateway                  ] [Assassin] successful state recovery, importing cluster state...
[2013-01-18 18:31:08,820][DEBUG][cluster.service          ] [Assassin] processing [local-gateway-elected-state]: execute
[2013-01-18 18:31:08,820][DEBUG][gateway.local            ] [Assassin] [cla.nine.dk][4]: allocating [[cla.nine.dk][4], node[null], [P], s[UNASSIGNED]] to [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]] on primary allocation
[2013-01-18 18:31:08,820][DEBUG][gateway.local            ] [Assassin] [cla.nine.dk][3]: allocating [[cla.nine.dk][3], node[null], [P], s[UNASSIGNED]] to [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]] on primary allocation
[2013-01-18 18:31:08,820][DEBUG][gateway.local            ] [Assassin] [cla.nine.dk][2]: allocating [[cla.nine.dk][2], node[null], [P], s[UNASSIGNED]] to [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]] on primary allocation
[2013-01-18 18:31:08,836][DEBUG][gateway.local            ] [Assassin] [cla.nine.dk][0]: allocating [[cla.nine.dk][0], node[null], [P], s[UNASSIGNED]] to [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]] on primary allocation
[2013-01-18 18:31:08,836][DEBUG][gateway.local            ] [Assassin] [cla.nine.dk][1]: throttling allocation [[cla.nine.dk][1], node[null], [P], s[UNASSIGNED]] to [[[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]]] on primary allocation
[2013-01-18 18:31:08,836][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [2], source [local-gateway-elected-state]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [P], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [P], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 18:31:08,836][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 18:31:08,836][DEBUG][indices.cluster          ] [Assassin] [cla.nine.dk] creating index
[2013-01-18 18:31:08,836][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 18:31:08,836][DEBUG][indices                  ] [Assassin] creating Index [cla.nine.dk], shards [5]/[1]
[2013-01-18 18:31:08,992][DEBUG][index.mapper             ] [Assassin] [cla.nine.dk] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/C:/dev/git/whalewired/whalewired.tools/elasticsearch-0.20.2/lib/elasticsearch-0.20.2.jar!/org/elasticsearch/index/mapper/default-mapping.json] and source[{
    "_default_":{
    }
}]
[2013-01-18 18:31:08,992][DEBUG][index.cache.field.data.resident] [Assassin] [cla.nine.dk] using [resident] field cache with max_size [-1], expire [null]
[2013-01-18 18:31:08,992][DEBUG][index.cache.query.parser.resident] [Assassin] [cla.nine.dk] using [resident] query cache with max_size [100], expire [null]
[2013-01-18 18:31:09,008][DEBUG][index.cache              ] [Assassin] [cla.nine.dk] Using stats.refresh_interval [1s]
[2013-01-18 18:31:09,008][DEBUG][index.store.fs           ] [Assassin] [cla.nine.dk] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]
[2013-01-18 18:31:09,008][DEBUG][indices.cluster          ] [Assassin] [cla.nine.dk] adding mapping [logevent], source [{"logevent":{"properties":{"accountName":{"type":"string"},"applicationName":{"type":"string"},"clientVersion":{"type":"string"},"hostName":{"type":"string"},"logFileName":{"type":"string"},"logLevel":{"type":"string","index":"not_analyzed","omit_norms":true,"index_options":"docs"},"logLineNumber":{"type":"string"},"logLocation":{"type":"string"},"logLocationShort":{"type":"string"},"logMessage":{"type":"multi_field","fields":{"logMessage":{"type":"string"},"untouched":{"type":"string","index":"not_analyzed","omit_norms":true,"index_options":"docs","include_in_all":false}}},"logMethodName":{"type":"string"},"logQualifiedClassName":{"type":"string"},"logThread":{"type":"string"},"logThrowableLocation":{"type":"string"},"logThrowableTrace":{"type":"string"},"logThrowableType":{"type":"string"},"logTime":{"type":"date","format":"dateOptionalTime"},"loggerName":{"type":"string"}}}}]
[2013-01-18 18:31:09,039][INFO ][http                     ] [Assassin] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.34:9200]}
[2013-01-18 18:31:09,039][INFO ][node                     ] [Assassin] {0.20.2}[4884]: started
[2013-01-18 18:31:09,070][DEBUG][indices.cluster          ] [Assassin] [cla.nine.dk][0] creating shard
[2013-01-18 18:31:09,070][DEBUG][index.service            ] [Assassin] [cla.nine.dk] creating shard_id [0]
[2013-01-18 18:31:09,148][DEBUG][index.store              ] [Assassin] [cla.nine.dk][0] using compress.stored [false], compress.tv [false]
[2013-01-18 18:31:09,148][DEBUG][index.deletionpolicy     ] [Assassin] [cla.nine.dk][0] Using [keep_only_last] deletion policy
[2013-01-18 18:31:09,164][DEBUG][index.merge.policy       ] [Assassin] [cla.nine.dk][0] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0], async_merge[true]
[2013-01-18 18:31:09,164][DEBUG][index.merge.scheduler    ] [Assassin] [cla.nine.dk][0] using [concurrent] merge scheduler with max_thread_count[3]
[2013-01-18 18:31:09,164][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][0] state: [CREATED]
[2013-01-18 18:31:09,164][DEBUG][index.translog           ] [Assassin] [cla.nine.dk][0] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2013-01-18 18:31:09,164][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][0] state: [CREATED]->[RECOVERING], reason [from gateway]
[2013-01-18 18:31:09,164][DEBUG][indices.cluster          ] [Assassin] [cla.nine.dk][2] creating shard
[2013-01-18 18:31:09,164][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][0] starting recovery from local ...
[2013-01-18 18:31:09,164][DEBUG][index.service            ] [Assassin] [cla.nine.dk] creating shard_id [2]
[2013-01-18 18:31:09,179][TRACE][index.gateway.local      ] [Assassin] [cla.nine.dk][0] using existing shard data, translog id [1357154525637]
[2013-01-18 18:31:09,179][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][0] starting engine
[2013-01-18 18:31:09,195][DEBUG][index.store              ] [Assassin] [cla.nine.dk][2] using compress.stored [false], compress.tv [false]
[2013-01-18 18:31:09,195][DEBUG][index.deletionpolicy     ] [Assassin] [cla.nine.dk][2] Using [keep_only_last] deletion policy
[2013-01-18 18:31:09,195][DEBUG][index.merge.policy       ] [Assassin] [cla.nine.dk][2] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0], async_merge[true]
[2013-01-18 18:31:09,195][DEBUG][index.merge.scheduler    ] [Assassin] [cla.nine.dk][2] using [concurrent] merge scheduler with max_thread_count[3]
[2013-01-18 18:31:09,195][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][2] state: [CREATED]
[2013-01-18 18:31:09,195][DEBUG][index.translog           ] [Assassin] [cla.nine.dk][2] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2013-01-18 18:31:09,195][DEBUG][indices.memory           ] [Assassin] recalculating shard indexing buffer (reason=created_shard[cla.nine.dk][2]), total is [98.9mb] with [1] active shards, each shard set to [98.9mb]
[2013-01-18 18:31:09,242][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][0] using bloom filter enhanced delete handling
[2013-01-18 18:31:09,288][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][0] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C167)], new [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C167)]
[2013-01-18 18:31:09,288][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][0] warming took [800.9micros]
[2013-01-18 18:31:09,288][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][0] updating index_buffer_size from [64mb] to [98.9mb]
[2013-01-18 18:31:09,288][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][2] state: [CREATED]->[RECOVERING], reason [from gateway]
[2013-01-18 18:31:09,288][DEBUG][indices.cluster          ] [Assassin] [cla.nine.dk][3] creating shard
[2013-01-18 18:31:09,288][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][2] starting recovery from local ...
[2013-01-18 18:31:09,288][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][0] state: [RECOVERING]->[STARTED], reason [post recovery]
[2013-01-18 18:31:09,288][DEBUG][index.service            ] [Assassin] [cla.nine.dk] creating shard_id [3]
[2013-01-18 18:31:09,288][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][0] scheduling refresher every 1s
[2013-01-18 18:31:09,288][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][0] scheduling optimizer / merger every 1s
[2013-01-18 18:31:09,288][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][0] refresh with waitForOperations[false]
[2013-01-18 18:31:09,288][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][0] recovery completed from local, took [124ms]
    index    : files           [12] with total_size [441.8kb], took[15ms]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [12] with total_size [441.8kb]
    start    : took [109ms], check_index [0s]
    translog : number_of_operations [0], took [0s]
[2013-01-18 18:31:09,304][DEBUG][cluster.action.shard     ] [Assassin] sending shard started for [cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,304][DEBUG][cluster.action.shard     ] [Assassin] received shard started for [cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,288][TRACE][index.gateway.local      ] [Assassin] [cla.nine.dk][2] using existing shard data, translog id [1357154525824]
[2013-01-18 18:31:09,304][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][2] starting engine
[2013-01-18 18:31:09,304][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][2] using bloom filter enhanced delete handling
[2013-01-18 18:31:09,320][DEBUG][index.store              ] [Assassin] [cla.nine.dk][3] using compress.stored [false], compress.tv [false]
[2013-01-18 18:31:09,320][DEBUG][index.deletionpolicy     ] [Assassin] [cla.nine.dk][3] Using [keep_only_last] deletion policy
[2013-01-18 18:31:09,320][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][2] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C187)], new [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C187)]
[2013-01-18 18:31:09,320][DEBUG][index.merge.policy       ] [Assassin] [cla.nine.dk][3] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0], async_merge[true]
[2013-01-18 18:31:09,320][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][2] warming took [50.8micros]
[2013-01-18 18:31:09,320][DEBUG][index.merge.scheduler    ] [Assassin] [cla.nine.dk][3] using [concurrent] merge scheduler with max_thread_count[3]
[2013-01-18 18:31:09,320][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][3] state: [CREATED]
[2013-01-18 18:31:09,320][DEBUG][index.translog           ] [Assassin] [cla.nine.dk][3] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2013-01-18 18:31:09,320][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][2] state: [RECOVERING]->[STARTED], reason [post recovery]
[2013-01-18 18:31:09,335][DEBUG][indices.memory           ] [Assassin] recalculating shard indexing buffer (reason=created_shard[cla.nine.dk][3]), total is [98.9mb] with [2] active shards, each shard set to [49.4mb]
[2013-01-18 18:31:09,335][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][2] scheduling refresher every 1s
[2013-01-18 18:31:09,335][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][0] updating index_buffer_size from [98.9mb] to [49.4mb]
[2013-01-18 18:31:09,335][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][2] scheduling optimizer / merger every 1s
[2013-01-18 18:31:09,335][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][2] updating index_buffer_size from [64mb] to [49.4mb]
[2013-01-18 18:31:09,335][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][2] refresh with waitForOperations[false]
[2013-01-18 18:31:09,335][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][3] state: [CREATED]->[RECOVERING], reason [from gateway]
[2013-01-18 18:31:09,335][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][2] recovery completed from local, took [47ms]
    index    : files           [12] with total_size [466.9kb], took[16ms]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [12] with total_size [466.9kb]
    start    : took [16ms], check_index [0s]
    translog : number_of_operations [0], took [15ms]
[2013-01-18 18:31:09,335][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][3] starting recovery from local ...
[2013-01-18 18:31:09,335][DEBUG][indices.cluster          ] [Assassin] [cla.nine.dk][4] creating shard
[2013-01-18 18:31:09,335][DEBUG][cluster.action.shard     ] [Assassin] sending shard started for [cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,335][DEBUG][index.service            ] [Assassin] [cla.nine.dk] creating shard_id [4]
[2013-01-18 18:31:09,335][DEBUG][cluster.action.shard     ] [Assassin] received shard started for [cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,335][TRACE][index.gateway.local      ] [Assassin] [cla.nine.dk][3] using existing shard data, translog id [1357154525715]
[2013-01-18 18:31:09,335][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][3] starting engine
[2013-01-18 18:31:09,351][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][3] using bloom filter enhanced delete handling
[2013-01-18 18:31:09,366][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][3] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C165)], new [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C165)]
[2013-01-18 18:31:09,366][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][3] warming took [102.1micros]
[2013-01-18 18:31:09,366][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][3] state: [RECOVERING]->[STARTED], reason [post recovery]
[2013-01-18 18:31:09,366][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][3] scheduling refresher every 1s
[2013-01-18 18:31:09,366][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][3] scheduling optimizer / merger every 1s
[2013-01-18 18:31:09,366][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][3] refresh with waitForOperations[false]
[2013-01-18 18:31:09,366][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][3] recovery completed from local, took [31ms]
    index    : files           [12] with total_size [428kb], took[0s]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [12] with total_size [428kb]
    start    : took [31ms], check_index [0s]
    translog : number_of_operations [0], took [0s]
[2013-01-18 18:31:09,366][DEBUG][cluster.action.shard     ] [Assassin] sending shard started for [cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,366][DEBUG][cluster.action.shard     ] [Assassin] received shard started for [cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,366][DEBUG][index.store              ] [Assassin] [cla.nine.dk][4] using compress.stored [false], compress.tv [false]
[2013-01-18 18:31:09,366][DEBUG][index.deletionpolicy     ] [Assassin] [cla.nine.dk][4] Using [keep_only_last] deletion policy
[2013-01-18 18:31:09,366][DEBUG][index.merge.policy       ] [Assassin] [cla.nine.dk][4] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0], async_merge[true]
[2013-01-18 18:31:09,366][DEBUG][index.merge.scheduler    ] [Assassin] [cla.nine.dk][4] using [concurrent] merge scheduler with max_thread_count[3]
[2013-01-18 18:31:09,366][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][4] state: [CREATED]
[2013-01-18 18:31:09,366][DEBUG][index.translog           ] [Assassin] [cla.nine.dk][4] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2013-01-18 18:31:09,382][DEBUG][indices.memory           ] [Assassin] recalculating shard indexing buffer (reason=created_shard[cla.nine.dk][4]), total is [98.9mb] with [3] active shards, each shard set to [32.9mb]
[2013-01-18 18:31:09,382][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][0] updating index_buffer_size from [49.4mb] to [32.9mb]
[2013-01-18 18:31:09,382][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][2] updating index_buffer_size from [49.4mb] to [32.9mb]
[2013-01-18 18:31:09,382][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][3] updating index_buffer_size from [64mb] to [32.9mb]
[2013-01-18 18:31:09,382][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][4] state: [CREATED]->[RECOVERING], reason [from gateway]
[2013-01-18 18:31:09,382][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][4] starting recovery from local ...
[2013-01-18 18:31:09,382][TRACE][gateway.local.state.meta ] [Assassin] [_global] writing state, reason [changed]
[2013-01-18 18:31:09,382][TRACE][index.gateway.local      ] [Assassin] [cla.nine.dk][4] using existing shard data, translog id [1357154525762]
[2013-01-18 18:31:09,382][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][4] starting engine
[2013-01-18 18:31:09,382][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][4] using bloom filter enhanced delete handling
[2013-01-18 18:31:09,382][INFO ][gateway                  ] [Assassin] recovered [1] indices into cluster_state
[2013-01-18 18:31:09,382][DEBUG][cluster.service          ] [Assassin] processing [local-gateway-elected-state]: done applying updated cluster_state
[2013-01-18 18:31:09,382][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute
[2013-01-18 18:31:09,382][DEBUG][cluster.action.shard     ] [Assassin] applying started shards [[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], [cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], [cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]], reason [after recovery from gateway]
[2013-01-18 18:31:09,398][DEBUG][gateway.local            ] [Assassin] [cla.nine.dk][1]: allocating [[cla.nine.dk][1], node[null], [P], s[UNASSIGNED]] to [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]] on primary allocation
[2013-01-18 18:31:09,398][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [3], source [shard-started ([cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 18:31:09,398][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 18:31:09,398][DEBUG][indices.cluster          ] [Assassin] [cla.nine.dk][1] creating shard
[2013-01-18 18:31:09,398][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 18:31:09,398][DEBUG][index.service            ] [Assassin] [cla.nine.dk] creating shard_id [1]
[2013-01-18 18:31:09,398][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][4] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C156)], new [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C156)]
[2013-01-18 18:31:09,398][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][4] warming took [65.1micros]
[2013-01-18 18:31:09,413][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][4] state: [RECOVERING]->[STARTED], reason [post recovery]
[2013-01-18 18:31:09,413][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][4] scheduling refresher every 1s
[2013-01-18 18:31:09,413][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][4] scheduling optimizer / merger every 1s
[2013-01-18 18:31:09,413][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][4] refresh with waitForOperations[false]
[2013-01-18 18:31:09,413][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][4] recovery completed from local, took [31ms]
    index    : files           [12] with total_size [396.3kb], took[0s]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [12] with total_size [396.3kb]
    start    : took [16ms], check_index [0s]
    translog : number_of_operations [0], took [15ms]
[2013-01-18 18:31:09,413][DEBUG][cluster.action.shard     ] [Assassin] sending shard started for [cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,413][DEBUG][cluster.action.shard     ] [Assassin] received shard started for [cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,429][DEBUG][index.store              ] [Assassin] [cla.nine.dk][1] using compress.stored [false], compress.tv [false]
[2013-01-18 18:31:09,429][DEBUG][index.deletionpolicy     ] [Assassin] [cla.nine.dk][1] Using [keep_only_last] deletion policy
[2013-01-18 18:31:09,429][DEBUG][index.merge.policy       ] [Assassin] [cla.nine.dk][1] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0], async_merge[true]
[2013-01-18 18:31:09,429][DEBUG][index.merge.scheduler    ] [Assassin] [cla.nine.dk][1] using [concurrent] merge scheduler with max_thread_count[3]
[2013-01-18 18:31:09,429][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][1] state: [CREATED]
[2013-01-18 18:31:09,429][DEBUG][index.translog           ] [Assassin] [cla.nine.dk][1] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2013-01-18 18:31:09,429][DEBUG][indices.memory           ] [Assassin] recalculating shard indexing buffer (reason=created_shard[cla.nine.dk][1]), total is [98.9mb] with [4] active shards, each shard set to [24.7mb]
[2013-01-18 18:31:09,429][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][0] updating index_buffer_size from [32.9mb] to [24.7mb]
[2013-01-18 18:31:09,429][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][2] updating index_buffer_size from [32.9mb] to [24.7mb]
[2013-01-18 18:31:09,429][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][3] updating index_buffer_size from [32.9mb] to [24.7mb]
[2013-01-18 18:31:09,429][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][4] updating index_buffer_size from [64mb] to [24.7mb]
[2013-01-18 18:31:09,429][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][1] state: [CREATED]->[RECOVERING], reason [from gateway]
[2013-01-18 18:31:09,444][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][1] starting recovery from local ...
[2013-01-18 18:31:09,444][TRACE][indices.cluster          ] [Assassin] [{}][{}] master [{}] marked shard as initializing, but shard already created, mark shard as started
[2013-01-18 18:31:09,444][DEBUG][cluster.action.shard     ] [Assassin] sending shard started for [cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [master [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]] marked shard as initializing, but shard already started, mark shard as started]
[2013-01-18 18:31:09,444][DEBUG][cluster.action.shard     ] [Assassin] received shard started for [cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [master [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]] marked shard as initializing, but shard already started, mark shard as started]
[2013-01-18 18:31:09,444][TRACE][gateway.local.state.shards] [Assassin] [cla.nine.dk][0] writing shard state, reason [version changed from [4] to [6]]
[2013-01-18 18:31:09,444][TRACE][index.gateway.local      ] [Assassin] [cla.nine.dk][1] using existing shard data, translog id [1357154525684]
[2013-01-18 18:31:09,444][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][1] starting engine
[2013-01-18 18:31:09,444][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][1] using bloom filter enhanced delete handling
[2013-01-18 18:31:09,460][TRACE][gateway.local.state.shards] [Assassin] [cla.nine.dk][2] writing shard state, reason [version changed from [4] to [6]]
[2013-01-18 18:31:09,460][TRACE][gateway.local.state.shards] [Assassin] [cla.nine.dk][3] writing shard state, reason [version changed from [4] to [6]]
[2013-01-18 18:31:09,460][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: done applying updated cluster_state
[2013-01-18 18:31:09,460][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute
[2013-01-18 18:31:09,460][DEBUG][cluster.action.shard     ] [Assassin] applying started shards [[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], [cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]], reason [after recovery from gateway]
[2013-01-18 18:31:09,460][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [4], source [shard-started ([cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 18:31:09,460][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 18:31:09,460][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 18:31:09,460][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][1] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C158)], new [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C158)]
[2013-01-18 18:31:09,460][TRACE][gateway.local.state.shards] [Assassin] [cla.nine.dk][4] writing shard state, reason [version changed from [4] to [6]]
[2013-01-18 18:31:09,460][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][1] warming took [66micros]
[2013-01-18 18:31:09,476][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: done applying updated cluster_state
[2013-01-18 18:31:09,476][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute
[2013-01-18 18:31:09,476][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][1] state: [RECOVERING]->[STARTED], reason [post recovery]
[2013-01-18 18:31:09,476][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: no change in cluster_state
[2013-01-18 18:31:09,476][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][1] scheduling refresher every 1s
[2013-01-18 18:31:09,476][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute
[2013-01-18 18:31:09,476][DEBUG][index.shard.service      ] [Assassin] [cla.nine.dk][1] scheduling optimizer / merger every 1s
[2013-01-18 18:31:09,476][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: no change in cluster_state
[2013-01-18 18:31:09,476][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][1] refresh with waitForOperations[false]
[2013-01-18 18:31:09,476][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [master [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]] marked shard as initializing, but shard already started, mark shard as started]]: execute
[2013-01-18 18:31:09,476][DEBUG][index.gateway            ] [Assassin] [cla.nine.dk][1] recovery completed from local, took [32ms]
    index    : files           [12] with total_size [408.1kb], took[0s]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [12] with total_size [408.1kb]
    start    : took [32ms], check_index [0s]
    translog : number_of_operations [0], took [0s]
[2013-01-18 18:31:09,476][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [master [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]] marked shard as initializing, but shard already started, mark shard as started]]: no change in cluster_state
[2013-01-18 18:31:09,476][DEBUG][cluster.action.shard     ] [Assassin] sending shard started for [cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,476][DEBUG][cluster.action.shard     ] [Assassin] received shard started for [cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING], reason [after recovery from gateway]
[2013-01-18 18:31:09,476][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute
[2013-01-18 18:31:09,491][DEBUG][cluster.action.shard     ] [Assassin] applying started shards [[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]], reason [after recovery from gateway]
[2013-01-18 18:31:09,491][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [5], source [shard-started ([cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 18:31:09,491][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 18:31:09,491][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 18:31:09,491][TRACE][gateway.local.state.shards] [Assassin] [cla.nine.dk][1] writing shard state, reason [version changed from [4] to [6]]
[2013-01-18 18:31:09,491][DEBUG][cluster.service          ] [Assassin] processing [shard-started ([cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: done applying updated cluster_state
[2013-01-18 18:31:18,820][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: execute
[2013-01-18 18:31:18,820][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: no change in cluster_state
[2013-01-18 18:31:38,833][TRACE][discovery.zen.ping.multicast] [Assassin] [1] received ping_request from [[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}], sending ping_response{target [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], master [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], cluster_name[whalewired_cluster]}
[2013-01-18 18:31:38,833][DEBUG][transport.netty          ] [Assassin] connected to node [[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}]
[2013-01-18 18:31:40,346][TRACE][discovery.zen.ping.multicast] [Assassin] [1] received ping_request from [[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}], sending ping_response{target [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], master [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], cluster_name[whalewired_cluster]}
[2013-01-18 18:31:41,859][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xd5ffe017, /192.168.1.34:55380 => /192.168.1.34:9300]
[2013-01-18 18:31:41,859][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x2244ad77, /192.168.1.34:55381 => /192.168.1.34:9300]
[2013-01-18 18:31:41,859][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xd8495f6d, /192.168.1.34:55382 => /192.168.1.34:9300]
[2013-01-18 18:31:41,859][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x051164a6, /192.168.1.34:55383 => /192.168.1.34:9300]
[2013-01-18 18:31:41,859][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x686aa69e, /192.168.1.34:55384 => /192.168.1.34:9300]
[2013-01-18 18:31:41,859][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x73a1fe75, /192.168.1.34:55385 => /192.168.1.34:9300]
[2013-01-18 18:31:41,859][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x7fa6b039, /192.168.1.34:55386 => /192.168.1.34:9300]
[2013-01-18 18:31:41,859][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x6ad59e13, /192.168.1.34:55387 => /192.168.1.34:9300]
[2013-01-18 18:31:41,875][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xf3905814, /192.168.1.34:55388 => /192.168.1.34:9300]
[2013-01-18 18:31:41,922][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-receive(join from node[[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}])]: execute
[2013-01-18 18:31:41,922][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [6], source [zen-disco-receive(join from node[[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}])]
nodes: 
   [Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 18:31:41,937][INFO ][cluster.service          ] [Assassin] added {[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false},}, reason: zen-disco-receive(join from node[[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}])
[2013-01-18 18:31:41,937][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 18:31:41,937][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-receive(join from node[[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}])]: done applying updated cluster_state
[2013-01-18 18:31:41,937][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 18:32:06,573][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C187], took [29.6ms], took_millis [29]
[2013-01-18 18:32:06,573][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C165], took [29.6ms], took_millis [29]
[2013-01-18 18:32:06,573][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C158], took [29.6ms], took_millis [29]
[2013-01-18 18:32:06,573][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C156], took [29.5ms], took_millis [29]
[2013-01-18 18:32:06,573][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C167], took [29.5ms], took_millis [29]
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x7fa6b039, /192.168.1.34:55386 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x051164a6, /192.168.1.34:55383 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,715][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x051164a6, /192.168.1.34:55383 => /192.168.1.34:9300]
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xeb621821, /192.168.1.34:55377 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x73a1fe75, /192.168.1.34:55385 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xd5ffe017, /192.168.1.34:55380 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,715][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xd5ffe017, /192.168.1.34:55380 => /192.168.1.34:9300]
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x2f52b9fb, /192.168.1.34:55379 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xf3905814, /192.168.1.34:55388 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x9f1114eb, /192.168.1.34:55374 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xd8495f6d, /192.168.1.34:55382 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xcaf664ec, /192.168.1.34:55378 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xa4bab638, /192.168.1.34:55375 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x164bd781, /192.168.1.34:55371 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x2f29eb85, /192.168.1.34:55373 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x686aa69e, /192.168.1.34:55384 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x331e1843, /192.168.1.34:55376 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x49fe2417, /192.168.1.34:55372 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x6ad59e13, /192.168.1.34:55387 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,699][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x2244ad77, /192.168.1.34:55381 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:38,762][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x6ad59e13, /192.168.1.34:55387 => /192.168.1.34:9300]
[2013-01-18 18:40:38,747][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x686aa69e, /192.168.1.34:55384 => /192.168.1.34:9300]
[2013-01-18 18:40:38,731][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xd8495f6d, /192.168.1.34:55382 => /192.168.1.34:9300]
[2013-01-18 18:40:38,731][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xf3905814, /192.168.1.34:55388 => /192.168.1.34:9300]
[2013-01-18 18:40:38,730][DEBUG][transport.netty          ] [Assassin] disconnected from [[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}]
[2013-01-18 18:40:38,715][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x73a1fe75, /192.168.1.34:55385 => /192.168.1.34:9300]
[2013-01-18 18:40:38,715][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x7fa6b039, /192.168.1.34:55386 => /192.168.1.34:9300]
[2013-01-18 18:40:38,762][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x2244ad77, /192.168.1.34:55381 => /192.168.1.34:9300]
[2013-01-18 18:40:39,792][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0xdf712c3f]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 18:40:39,792][TRACE][discovery.zen.fd         ] [Assassin] [node  ] [[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}] transport disconnected (with verified connect)
[2013-01-18 18:40:39,792][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-node_failed([Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]: execute
[2013-01-18 18:40:39,792][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [7], source [zen-disco-node_failed([Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 18:40:39,792][INFO ][cluster.service          ] [Assassin] removed {[Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false},}, reason: zen-disco-node_failed([Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)
[2013-01-18 18:40:39,792][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 18:40:39,792][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 18:40:39,792][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-node_failed([Balor][LaVtWBQcQESIx12jWzEYhA][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]: done applying updated cluster_state
[2013-01-18 18:40:39,792][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: execute
[2013-01-18 18:40:39,792][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: no change in cluster_state
[2013-01-18 19:01:11,598][TRACE][index.translog           ] [Assassin] [cla.nine.dk][0] flushing translog, last_flush_time [1358530269164], breached [30m]
[2013-01-18 19:01:11,598][TRACE][index.translog           ] [Assassin] [cla.nine.dk][2] flushing translog, last_flush_time [1358530269195], breached [30m]
[2013-01-18 19:01:11,598][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][0] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:01:11,598][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][2] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:01:11,660][TRACE][index.translog           ] [Assassin] [cla.nine.dk][3] flushing translog, last_flush_time [1358530269320], breached [30m]
[2013-01-18 19:01:11,660][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][3] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:01:11,691][TRACE][index.translog           ] [Assassin] [cla.nine.dk][4] flushing translog, last_flush_time [1358530269366], breached [30m]
[2013-01-18 19:01:11,691][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][4] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:01:11,800][TRACE][index.translog           ] [Assassin] [cla.nine.dk][1] flushing translog, last_flush_time [1358530269429], breached [30m]
[2013-01-18 19:01:11,800][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][1] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:02:05,501][DEBUG][indices.memory           ] [Assassin] marking shard [cla.nine.dk][0] as inactive (inactive_time[30m]) indexing wise, setting size to [500kb]
[2013-01-18 19:02:05,501][DEBUG][indices.memory           ] [Assassin] marking shard [cla.nine.dk][1] as inactive (inactive_time[30m]) indexing wise, setting size to [500kb]
[2013-01-18 19:02:05,501][DEBUG][indices.memory           ] [Assassin] marking shard [cla.nine.dk][2] as inactive (inactive_time[30m]) indexing wise, setting size to [500kb]
[2013-01-18 19:02:05,501][DEBUG][indices.memory           ] [Assassin] marking shard [cla.nine.dk][3] as inactive (inactive_time[30m]) indexing wise, setting size to [500kb]
[2013-01-18 19:02:05,501][DEBUG][indices.memory           ] [Assassin] marking shard [cla.nine.dk][4] as inactive (inactive_time[30m]) indexing wise, setting size to [500kb]
[2013-01-18 19:02:05,501][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][0] updating index_buffer_size from [24.7mb] to (inactive) [500kb]
[2013-01-18 19:02:05,516][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][0] using bloom filter enhanced delete handling
[2013-01-18 19:02:05,516][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][0] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C167)], new [MultiReader(_0(3.6.2):C167)]
[2013-01-18 19:02:05,516][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][0] warming took [679.5micros]
[2013-01-18 19:02:05,516][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][1] updating index_buffer_size from [64mb] to (inactive) [500kb]
[2013-01-18 19:02:05,532][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][1] using bloom filter enhanced delete handling
[2013-01-18 19:02:05,532][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][1] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C158)], new [MultiReader(_0(3.6.2):C158)]
[2013-01-18 19:02:05,532][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][1] warming took [120micros]
[2013-01-18 19:02:05,532][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][2] updating index_buffer_size from [24.7mb] to (inactive) [500kb]
[2013-01-18 19:02:05,547][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][2] using bloom filter enhanced delete handling
[2013-01-18 19:02:05,547][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][2] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C187)], new [MultiReader(_0(3.6.2):C187)]
[2013-01-18 19:02:05,547][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][2] warming took [76.7micros]
[2013-01-18 19:02:05,547][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][3] updating index_buffer_size from [24.7mb] to (inactive) [500kb]
[2013-01-18 19:02:05,547][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][3] using bloom filter enhanced delete handling
[2013-01-18 19:02:05,547][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][3] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C165)], new [MultiReader(_0(3.6.2):C165)]
[2013-01-18 19:02:05,547][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][3] warming took [101.7micros]
[2013-01-18 19:02:05,547][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][4] updating index_buffer_size from [24.7mb] to (inactive) [500kb]
[2013-01-18 19:02:05,563][DEBUG][index.engine.robin       ] [Assassin] [cla.nine.dk][4] using bloom filter enhanced delete handling
[2013-01-18 19:02:05,563][TRACE][indices.warmer           ] [Assassin] [cla.nine.dk][4] warming [ReadOnlyDirectoryReader(segments_2:nrt _0(3.6.2):C156)], new [MultiReader(_0(3.6.2):C156)]
[2013-01-18 19:02:05,563][TRACE][index.warmer             ] [Assassin] [cla.nine.dk][4] warming took [66.9micros]
[2013-01-18 19:31:14,087][TRACE][index.translog           ] [Assassin] [cla.nine.dk][0] flushing translog, last_flush_time [1358532071551], breached [30m]
[2013-01-18 19:31:14,087][TRACE][index.translog           ] [Assassin] [cla.nine.dk][2] flushing translog, last_flush_time [1358532071551], breached [30m]
[2013-01-18 19:31:14,087][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][0] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:31:14,087][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][2] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:31:14,134][TRACE][index.translog           ] [Assassin] [cla.nine.dk][3] flushing translog, last_flush_time [1358532071551], breached [30m]
[2013-01-18 19:31:14,134][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][3] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:31:14,196][TRACE][index.translog           ] [Assassin] [cla.nine.dk][4] flushing translog, last_flush_time [1358532071551], breached [30m]
[2013-01-18 19:31:14,196][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][4] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 19:31:14,306][TRACE][index.translog           ] [Assassin] [cla.nine.dk][1] flushing translog, last_flush_time [1358532071754], breached [30m]
[2013-01-18 19:31:14,306][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][1] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:01:15,947][TRACE][index.translog           ] [Assassin] [cla.nine.dk][0] flushing translog, last_flush_time [1358533873916], breached [30m]
[2013-01-18 20:01:15,947][TRACE][index.translog           ] [Assassin] [cla.nine.dk][2] flushing translog, last_flush_time [1358533873916], breached [30m]
[2013-01-18 20:01:15,947][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][0] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:01:15,947][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][2] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:01:15,962][TRACE][index.translog           ] [Assassin] [cla.nine.dk][3] flushing translog, last_flush_time [1358533874118], breached [30m]
[2013-01-18 20:01:15,962][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][3] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:01:16,056][TRACE][index.translog           ] [Assassin] [cla.nine.dk][4] flushing translog, last_flush_time [1358533874118], breached [30m]
[2013-01-18 20:01:16,056][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][4] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:01:16,134][TRACE][index.translog           ] [Assassin] [cla.nine.dk][1] flushing translog, last_flush_time [1358533874118], breached [30m]
[2013-01-18 20:01:16,134][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][1] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:19:12,347][TRACE][discovery.zen.ping.multicast] [Assassin] [1] received ping_request from [[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}], sending ping_response{target [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], master [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], cluster_name[whalewired_cluster]}
[2013-01-18 20:19:12,350][DEBUG][transport.netty          ] [Assassin] connected to node [[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}]
[2013-01-18 20:19:13,850][TRACE][discovery.zen.ping.multicast] [Assassin] [1] received ping_request from [[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}], sending ping_response{target [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], master [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], cluster_name[whalewired_cluster]}
[2013-01-18 20:19:15,361][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xf4d10ad9, /192.168.1.34:56141 => /192.168.1.34:9300]
[2013-01-18 20:19:15,364][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xfdf467e4, /192.168.1.34:56142 => /192.168.1.34:9300]
[2013-01-18 20:19:15,366][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x2442997a, /192.168.1.34:56143 => /192.168.1.34:9300]
[2013-01-18 20:19:15,367][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x1e4b1d60, /192.168.1.34:56144 => /192.168.1.34:9300]
[2013-01-18 20:19:15,368][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x92911bae, /192.168.1.34:56145 => /192.168.1.34:9300]
[2013-01-18 20:19:15,369][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xb878ba2b, /192.168.1.34:56146 => /192.168.1.34:9300]
[2013-01-18 20:19:15,370][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x42b0fa65, /192.168.1.34:56147 => /192.168.1.34:9300]
[2013-01-18 20:19:15,371][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xc68226f0, /192.168.1.34:56148 => /192.168.1.34:9300]
[2013-01-18 20:19:15,372][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x9efbdeb9, /192.168.1.34:56149 => /192.168.1.34:9300]
[2013-01-18 20:19:15,418][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-receive(join from node[[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}])]: execute
[2013-01-18 20:19:15,419][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [8], source [zen-disco-receive(join from node[[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}])]
nodes: 
   [Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 20:19:15,421][INFO ][cluster.service          ] [Assassin] added {[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false},}, reason: zen-disco-receive(join from node[[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}])
[2013-01-18 20:19:15,422][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 20:19:15,422][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 20:19:15,422][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-receive(join from node[[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}])]: done applying updated cluster_state
[2013-01-18 20:19:26,301][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C165], took [1.9ms], took_millis [1]
[2013-01-18 20:19:26,301][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C156], took [2.4ms], took_millis [2]
[2013-01-18 20:19:26,301][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C158], took [2.8ms], took_millis [2]
[2013-01-18 20:19:26,302][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C167], took [3.9ms], took_millis [3]
[2013-01-18 20:19:26,301][TRACE][index.cache.field.data.resident] [Assassin] [cla.nine.dk] loaded field [logTime] for reader [_0(3.6.2):C187], took [2.2ms], took_millis [2]
[2013-01-18 20:29:05,480][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x3d54234e, /192.168.1.34:56134 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,482][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x92911bae, /192.168.1.34:56145 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,490][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x92911bae, /192.168.1.34:56145 => /192.168.1.34:9300]
[2013-01-18 20:29:05,482][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x1e4b1d60, /192.168.1.34:56144 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,481][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xf549fc1c, /192.168.1.34:56138 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,482][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xb878ba2b, /192.168.1.34:56146 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,481][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x2442997a, /192.168.1.34:56143 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,481][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xc2e0f8b6, /192.168.1.34:56137 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,481][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xfdf467e4, /192.168.1.34:56142 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,481][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x42b0fa65, /192.168.1.34:56147 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,480][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xcbc005b4, /192.168.1.34:56135 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,480][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xec6748ea, /192.168.1.34:56133 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,480][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x8cfb5737, /192.168.1.34:56132 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,480][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x10e69b06, /192.168.1.34:56136 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,511][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x42b0fa65, /192.168.1.34:56147 => /192.168.1.34:9300]
[2013-01-18 20:29:05,509][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xfdf467e4, /192.168.1.34:56142 => /192.168.1.34:9300]
[2013-01-18 20:29:05,503][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x2442997a, /192.168.1.34:56143 => /192.168.1.34:9300]
[2013-01-18 20:29:05,500][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xb878ba2b, /192.168.1.34:56146 => /192.168.1.34:9300]
[2013-01-18 20:29:05,494][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x1e4b1d60, /192.168.1.34:56144 => /192.168.1.34:9300]
[2013-01-18 20:29:05,487][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xf4d10ad9, /192.168.1.34:56141 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,486][DEBUG][transport.netty          ] [Assassin] disconnected from [[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}]
[2013-01-18 20:29:05,486][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xc68226f0, /192.168.1.34:56148 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,521][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xc68226f0, /192.168.1.34:56148 => /192.168.1.34:9300]
[2013-01-18 20:29:05,485][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x9efbdeb9, /192.168.1.34:56149 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,522][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x9efbdeb9, /192.168.1.34:56149 => /192.168.1.34:9300]
[2013-01-18 20:29:05,485][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x2eaa540e, /192.168.1.34:56139 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:05,519][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xf4d10ad9, /192.168.1.34:56141 => /192.168.1.34:9300]
[2013-01-18 20:29:06,521][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0xf55cfd7b]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:06,525][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0xbaf9f7e1]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:06,528][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0x16aebd58]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:06,532][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0xdf22ff60]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:29:06,532][TRACE][discovery.zen.fd         ] [Assassin] [node  ] [[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}] transport disconnected (with verified connect)
[2013-01-18 20:29:06,535][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-node_failed([Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]: execute
[2013-01-18 20:29:06,536][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [9], source [zen-disco-node_failed([Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 20:29:06,539][INFO ][cluster.service          ] [Assassin] removed {[Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false},}, reason: zen-disco-node_failed([Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)
[2013-01-18 20:29:06,540][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 20:29:06,541][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 20:29:06,541][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-node_failed([Riot Grrl][LS28oS2NRb6YYxfSFKXiow][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]: done applying updated cluster_state
[2013-01-18 20:29:06,542][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: execute
[2013-01-18 20:29:06,543][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: no change in cluster_state
[2013-01-18 20:29:32,426][TRACE][discovery.zen.ping.multicast] [Assassin] [1] received ping_request from [[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}], sending ping_response{target [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], master [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], cluster_name[whalewired_cluster]}
[2013-01-18 20:29:32,428][DEBUG][transport.netty          ] [Assassin] connected to node [[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}]
[2013-01-18 20:29:33,929][TRACE][discovery.zen.ping.multicast] [Assassin] [1] received ping_request from [[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}], sending ping_response{target [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], master [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], cluster_name[whalewired_cluster]}
[2013-01-18 20:29:35,440][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x81b53774, /192.168.1.34:56302 => /192.168.1.34:9300]
[2013-01-18 20:29:35,443][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xea7e5c43, /192.168.1.34:56303 => /192.168.1.34:9300]
[2013-01-18 20:29:35,444][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xacf59ed1, /192.168.1.34:56304 => /192.168.1.34:9300]
[2013-01-18 20:29:35,445][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x4ab36b24, /192.168.1.34:56305 => /192.168.1.34:9300]
[2013-01-18 20:29:35,446][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x7cedc78b, /192.168.1.34:56306 => /192.168.1.34:9300]
[2013-01-18 20:29:35,447][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x21067c90, /192.168.1.34:56307 => /192.168.1.34:9300]
[2013-01-18 20:29:35,448][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x40a772ae, /192.168.1.34:56308 => /192.168.1.34:9300]
[2013-01-18 20:29:35,449][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x1b76e2e5, /192.168.1.34:56309 => /192.168.1.34:9300]
[2013-01-18 20:29:35,450][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xa34e3980, /192.168.1.34:56310 => /192.168.1.34:9300]
[2013-01-18 20:29:35,494][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-receive(join from node[[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}])]: execute
[2013-01-18 20:29:35,495][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [10], source [zen-disco-receive(join from node[[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}])]
nodes: 
   [Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 20:29:35,497][INFO ][cluster.service          ] [Assassin] added {[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false},}, reason: zen-disco-receive(join from node[[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}])
[2013-01-18 20:29:35,498][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 20:29:35,499][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 20:29:35,499][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-receive(join from node[[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}])]: done applying updated cluster_state
[2013-01-18 20:31:16,989][TRACE][index.translog           ] [Assassin] [cla.nine.dk][0] flushing translog, last_flush_time [1358535675900], breached [30m]
[2013-01-18 20:31:16,989][TRACE][index.translog           ] [Assassin] [cla.nine.dk][2] flushing translog, last_flush_time [1358535675900], breached [30m]
[2013-01-18 20:31:16,990][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][0] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:31:16,991][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][2] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:31:17,004][TRACE][index.translog           ] [Assassin] [cla.nine.dk][3] flushing translog, last_flush_time [1358535675900], breached [30m]
[2013-01-18 20:31:17,004][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][3] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:31:17,087][TRACE][index.translog           ] [Assassin] [cla.nine.dk][4] flushing translog, last_flush_time [1358535675900], breached [30m]
[2013-01-18 20:31:17,087][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][4] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:31:17,191][TRACE][index.translog           ] [Assassin] [cla.nine.dk][1] flushing translog, last_flush_time [1358535676103], breached [30m]
[2013-01-18 20:31:17,191][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][1] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 20:33:06,055][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x59703322, /192.168.1.34:56295 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,057][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xeeebffcc, /192.168.1.34:56297 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,057][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x21067c90, /192.168.1.34:56307 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,057][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xacf59ed1, /192.168.1.34:56304 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,057][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x1b76e2e5, /192.168.1.34:56309 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xe0d7bdaf, /192.168.1.34:56293 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x40a772ae, /192.168.1.34:56308 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x7551871e, /192.168.1.34:56294 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xa34e3980, /192.168.1.34:56310 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xe0552eb8, /192.168.1.34:56299 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x7cedc78b, /192.168.1.34:56306 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x4ab36b24, /192.168.1.34:56305 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x1d3a76af, /192.168.1.34:56298 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,056][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xea7e5c43, /192.168.1.34:56303 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,055][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x81b53774, /192.168.1.34:56302 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,055][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xf2215071, /192.168.1.34:56301 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,055][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x07c92714, /192.168.1.34:56300 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,055][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x82b81a97, /192.168.1.34:56296 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:06,094][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x81b53774, /192.168.1.34:56302 => /192.168.1.34:9300]
[2013-01-18 20:33:06,090][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xea7e5c43, /192.168.1.34:56303 => /192.168.1.34:9300]
[2013-01-18 20:33:06,087][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x4ab36b24, /192.168.1.34:56305 => /192.168.1.34:9300]
[2013-01-18 20:33:06,086][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x7cedc78b, /192.168.1.34:56306 => /192.168.1.34:9300]
[2013-01-18 20:33:06,083][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xa34e3980, /192.168.1.34:56310 => /192.168.1.34:9300]
[2013-01-18 20:33:06,079][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x40a772ae, /192.168.1.34:56308 => /192.168.1.34:9300]
[2013-01-18 20:33:06,073][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x1b76e2e5, /192.168.1.34:56309 => /192.168.1.34:9300]
[2013-01-18 20:33:06,070][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xacf59ed1, /192.168.1.34:56304 => /192.168.1.34:9300]
[2013-01-18 20:33:06,067][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x21067c90, /192.168.1.34:56307 => /192.168.1.34:9300]
[2013-01-18 20:33:06,062][DEBUG][transport.netty          ] [Assassin] disconnected from [[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}]
[2013-01-18 20:33:07,101][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0x74421c3b]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:07,104][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0x5f235489]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:07,106][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0x2c50b4cc]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:07,107][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0x27232e71]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:07,109][TRACE][transport.netty          ] [Assassin] (ignoring) exception caught on transport layer [[id: 0x331b4404]]
java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:33:07,109][TRACE][discovery.zen.fd         ] [Assassin] [node  ] [[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}] transport disconnected (with verified connect)
[2013-01-18 20:33:07,110][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-node_failed([Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]: execute
[2013-01-18 20:33:07,110][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [11], source [zen-disco-node_failed([Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 20:33:07,112][INFO ][cluster.service          ] [Assassin] removed {[Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false},}, reason: zen-disco-node_failed([Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)
[2013-01-18 20:33:07,112][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 20:33:07,112][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 20:33:07,113][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-node_failed([Amphibian][SZVe-r2sR6C8dQUZ3xEXeg][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]: done applying updated cluster_state
[2013-01-18 20:33:07,113][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: execute
[2013-01-18 20:33:07,113][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: no change in cluster_state
[2013-01-18 20:35:46,397][TRACE][discovery.zen.ping.multicast] [Assassin] [1] received ping_request from [[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}], sending ping_response{target [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], master [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], cluster_name[whalewired_cluster]}
[2013-01-18 20:35:46,400][DEBUG][transport.netty          ] [Assassin] connected to node [[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}]
[2013-01-18 20:35:47,899][TRACE][discovery.zen.ping.multicast] [Assassin] [1] received ping_request from [[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}], sending ping_response{target [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], master [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]], cluster_name[whalewired_cluster]}
[2013-01-18 20:35:49,409][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xf63f2f0d, /192.168.1.34:56457 => /192.168.1.34:9300]
[2013-01-18 20:35:49,412][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x5bb7dfe8, /192.168.1.34:56458 => /192.168.1.34:9300]
[2013-01-18 20:35:49,413][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xb4e6b01a, /192.168.1.34:56459 => /192.168.1.34:9300]
[2013-01-18 20:35:49,414][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x2051f5ab, /192.168.1.34:56460 => /192.168.1.34:9300]
[2013-01-18 20:35:49,415][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xe55f30ee, /192.168.1.34:56461 => /192.168.1.34:9300]
[2013-01-18 20:35:49,416][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xbda41ea4, /192.168.1.34:56462 => /192.168.1.34:9300]
[2013-01-18 20:35:49,417][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x16ccaed1, /192.168.1.34:56463 => /192.168.1.34:9300]
[2013-01-18 20:35:49,418][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x73d17fbf, /192.168.1.34:56464 => /192.168.1.34:9300]
[2013-01-18 20:35:49,419][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x6f3291b0, /192.168.1.34:56465 => /192.168.1.34:9300]
[2013-01-18 20:35:49,456][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-receive(join from node[[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}])]: execute
[2013-01-18 20:35:49,456][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [12], source [zen-disco-receive(join from node[[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}])]
nodes: 
   [Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 20:35:49,459][INFO ][cluster.service          ] [Assassin] added {[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false},}, reason: zen-disco-receive(join from node[[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}])
[2013-01-18 20:35:49,460][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 20:35:49,460][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-receive(join from node[[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}])]: done applying updated cluster_state
[2013-01-18 20:35:49,460][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 20:36:17,996][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x6e357b67, /192.168.1.34:55253 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,012][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x05a9104d, /192.168.1.34:55255 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,019][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x05a9104d, /192.168.1.34:55255 => /192.168.1.34:9300]
[2013-01-18 20:36:18,010][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x9ed95caa, /192.168.1.34:56453 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,024][DEBUG][transport.netty          ] [Assassin] disconnected from [[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}]
[2013-01-18 20:36:18,003][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x16ccaed1, /192.168.1.34:56463 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,003][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xfbb91528, /192.168.1.34:56455 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,000][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xbda41ea4, /192.168.1.34:56462 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,999][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x9623754b, /192.168.1.34:55257 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,069][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xbda41ea4, /192.168.1.34:56462 => /192.168.1.34:9300]
[2013-01-18 20:36:17,999][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x644b2965, /192.168.1.34:55254 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,076][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x644b2965, /192.168.1.34:55254 => /192.168.1.34:9300]
[2013-01-18 20:36:17,999][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x0adb2d76, /192.168.1.34:55256 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,998][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x5bb7dfe8, /192.168.1.34:56458 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,999][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x3d96c8e5, /192.168.1.34:55253 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,088][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x3d96c8e5, /192.168.1.34:55253 => /192.168.1.34:9300]
[2013-01-18 20:36:17,999][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x6f3291b0, /192.168.1.34:56465 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,099][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x6f3291b0, /192.168.1.34:56465 => /192.168.1.34:9300]
[2013-01-18 20:36:17,999][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xe33fb260, /192.168.1.34:56448 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,999][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x9aa94d54, /192.168.1.34:55255 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,999][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x5f05ee52, /192.168.1.34:55260 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,998][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x3b3bfc6a, /192.168.1.34:56456 => /192.168.1.34:9301]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,998][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x21c9b58c, /192.168.1.34:55254 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,998][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xb4e6b01a, /192.168.1.34:56459 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,998][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x2051f5ab, /192.168.1.34:56460 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,998][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x73d17fbf, /192.168.1.34:56464 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,126][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x73d17fbf, /192.168.1.34:56464 => /192.168.1.34:9300]
[2013-01-18 20:36:17,998][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xa4eb959f, /192.168.1.34:55252 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,998][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x445cae4f, /192.168.1.34:55260 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:17,997][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x811a3216, /192.168.1.34:55252 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,132][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x445cae4f, /192.168.1.34:55260 => /192.168.1.34:9300]
[2013-01-18 20:36:18,124][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x2051f5ab, /192.168.1.34:56460 => /192.168.1.34:9300]
[2013-01-18 20:36:18,121][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xb4e6b01a, /192.168.1.34:56459 => /192.168.1.34:9300]
[2013-01-18 20:36:18,083][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x5bb7dfe8, /192.168.1.34:56458 => /192.168.1.34:9300]
[2013-01-18 20:36:18,081][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x0adb2d76, /192.168.1.34:55256 => /192.168.1.34:9300]
[2013-01-18 20:36:18,141][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xf63f2f0d, /192.168.1.34:56457 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,143][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xf63f2f0d, /192.168.1.34:56457 => /192.168.1.34:9300]
[2013-01-18 20:36:18,047][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x16ccaed1, /192.168.1.34:56463 => /192.168.1.34:9300]
[2013-01-18 20:36:18,046][TRACE][discovery.zen.fd         ] [Assassin] [node  ] [[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}] transport disconnected (with verified connect)
[2013-01-18 20:36:18,038][WARN ][transport.netty          ] [Assassin] exception caught on transport layer [[id: 0x1fdf5fb4]], closing connection
java.net.SocketException: Network is unreachable: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,016][DEBUG][transport.netty          ] [Assassin] disconnected from [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]]
[2013-01-18 20:36:18,147][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-node_failed([Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]: execute
[2013-01-18 20:36:18,141][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x62a33fb8, /192.168.1.34:55257 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,154][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x62a33fb8, /192.168.1.34:55257 => /192.168.1.34:9300]
[2013-01-18 20:36:18,140][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x927c4c87, /192.168.1.34:55258 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,137][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0x61971f4e, /192.168.1.34:55259 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,136][TRACE][transport.netty          ] [Assassin] close connection exception caught on transport layer [[id: 0xe55f30ee, /192.168.1.34:56461 => /192.168.1.34:9300]], disconnecting from relevant node
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:36:18,165][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0xe55f30ee, /192.168.1.34:56461 => /192.168.1.34:9300]
[2013-01-18 20:36:18,135][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x811a3216, /192.168.1.34:55252 => /192.168.1.34:9300]
[2013-01-18 20:36:18,162][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x61971f4e, /192.168.1.34:55259 => /192.168.1.34:9300]
[2013-01-18 20:36:18,158][TRACE][transport.netty          ] [Assassin] channel closed: [id: 0x927c4c87, /192.168.1.34:55258 => /192.168.1.34:9300]
[2013-01-18 20:36:18,152][TRACE][cluster.service          ] [Assassin] cluster state updated:
version [13], source [zen-disco-node_failed([Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]
nodes: 
   [Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]], local, master
routing_table:
-- index [cla.nine.dk]
----shard_id [cla.nine.dk][0]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][1]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][2]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][3]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
----shard_id [cla.nine.dk][4]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[tJdQcBaOToW8ejPALNbsGQ][V]
--------[cla.nine.dk][0], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][1], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][2], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][3], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
--------[cla.nine.dk][4], node[tJdQcBaOToW8ejPALNbsGQ], [P], s[STARTED]
---- unassigned
--------[cla.nine.dk][0], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][1], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][2], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][3], node[null], [R], s[UNASSIGNED]
--------[cla.nine.dk][4], node[null], [R], s[UNASSIGNED]

[2013-01-18 20:36:18,173][INFO ][cluster.service          ] [Assassin] removed {[Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false},}, reason: zen-disco-node_failed([Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)
[2013-01-18 20:36:18,173][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: execute
[2013-01-18 20:36:18,174][DEBUG][river.cluster            ] [Assassin] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-01-18 20:36:18,175][DEBUG][cluster.service          ] [Assassin] processing [zen-disco-node_failed([Zero][zmMDRH-PQvayHXBr9GRthw][inet[/192.168.1.34:9301]]{client=true, data=false}), reason transport disconnected (with verified connect)]: done applying updated cluster_state
[2013-01-18 20:36:18,175][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: execute
[2013-01-18 20:36:18,176][DEBUG][cluster.service          ] [Assassin] processing [routing-table-updater]: no change in cluster_state
[2013-01-18 20:56:20,520][WARN ][transport.netty          ] [Assassin] exception caught on transport layer [[id: 0x97617307]], closing connection
java.net.SocketException: Network is unreachable: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
	at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
[2013-01-18 20:56:30,525][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x955bc418, /192.168.1.34:56550 => /192.168.1.34:9300]
[2013-01-18 20:56:30,525][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x43b758aa, /192.168.1.34:56551 => /192.168.1.34:9300]
[2013-01-18 20:56:30,526][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x0e08b338, /192.168.1.34:56552 => /192.168.1.34:9300]
[2013-01-18 20:56:30,526][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x91a19d93, /192.168.1.34:56553 => /192.168.1.34:9300]
[2013-01-18 20:56:30,526][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x42d6766d, /192.168.1.34:56554 => /192.168.1.34:9300]
[2013-01-18 20:56:30,527][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x43fa6977, /192.168.1.34:56555 => /192.168.1.34:9300]
[2013-01-18 20:56:30,527][DEBUG][transport.netty          ] [Assassin] connected to node [[Assassin][tJdQcBaOToW8ejPALNbsGQ][inet[/192.168.1.34:9300]]]
[2013-01-18 20:56:30,527][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0xf184a7e1, /192.168.1.34:56556 => /192.168.1.34:9300]
[2013-01-18 20:56:30,527][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x2bcaedde, /192.168.1.34:56557 => /192.168.1.34:9300]
[2013-01-18 20:56:30,528][TRACE][transport.netty          ] [Assassin] channel opened: [id: 0x81e0bf93, /192.168.1.34:56558 => /192.168.1.34:9300]
[2013-01-18 21:01:21,129][TRACE][index.translog           ] [Assassin] [cla.nine.dk][0] flushing translog, last_flush_time [1358537476818], breached [30m]
[2013-01-18 21:01:21,130][TRACE][index.translog           ] [Assassin] [cla.nine.dk][2] flushing translog, last_flush_time [1358537476818], breached [30m]
[2013-01-18 21:01:21,130][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][0] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 21:01:21,130][TRACE][index.translog           ] [Assassin] [cla.nine.dk][3] flushing translog, last_flush_time [1358537476818], breached [30m]
[2013-01-18 21:01:21,131][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][2] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 21:01:21,132][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][3] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 21:01:21,132][TRACE][index.translog           ] [Assassin] [cla.nine.dk][4] flushing translog, last_flush_time [1358537477018], breached [30m]
[2013-01-18 21:01:21,133][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][4] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
[2013-01-18 21:01:21,133][TRACE][index.translog           ] [Assassin] [cla.nine.dk][1] flushing translog, last_flush_time [1358537477018], breached [30m]
[2013-01-18 21:01:21,134][TRACE][index.shard.service      ] [Assassin] [cla.nine.dk][1] flush with type[COMMIT_TRANSLOG], refresh[false], force[false]
